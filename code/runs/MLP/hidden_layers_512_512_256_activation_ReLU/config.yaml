model:
  hidden_layers: [512, 512, 256]  # three hidden layers with 128, 64, and 32 nodes respectively
  activation_function: "ReLU" # options: "ReLU", "LeakyReLU", "ELU"
  use_batch_norm: True   # options: True, False
  dropout_rate: 0.1 # options: any float between 0 and 1, or None for no dropout
  regularization: "L1" # options: "L1", "L2", None for no regularization
  l1_lambda: 0.001
  l2_lambda: 0.001
  output_size: 47
  input_size: 784
optimizer:
  type: "Adam"  # options: "SGD", "Adam", "RMSprop", "ASGD", "AdaGrad"
  learning_rate: 0.001
  lr_scheduler: "StepLR"  # options: "StepLR", "ExponentialLR", None for no scheduler
  step_size: 10 # for StepLR
  gamma: 0.9 # for StepLR and ExponentialLR
n_epochs: 30
batch_size: 128
